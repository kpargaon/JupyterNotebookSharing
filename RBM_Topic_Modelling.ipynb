{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricted Boltzman Machine - Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "np_rng = np.random.RandomState(1234) #setting the random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "df = pd.read_excel(\"amazon.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  So there is no way for me to plug it in here i...\n",
       "1                        Good case, Excellent value.\n",
       "2                             Great for the jawbone.\n",
       "3  Tied to charger for conversations lasting more...\n",
       "4                                  The mic is great."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets run and check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words model on train and test data\n",
    "\n",
    "tf = CountVectorizer(\n",
    "    input='content',\n",
    "    encoding='utf-8',\n",
    "    decode_error='strict',\n",
    "    strip_accents=None,\n",
    "    lowercase=True,\n",
    "    preprocessor=None,\n",
    "    tokenizer=None,\n",
    "    stop_words=None,\n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=50,\n",
    "    min_df=1,\n",
    "    max_features=None,\n",
    "    vocabulary=None,\n",
    "    binary=False,\n",
    "    dtype=np.int64\n",
    ")\n",
    "\n",
    "# fit tf on the dataframe df\n",
    "tf.fit(df.Text)\n",
    "\n",
    "# transform df dataframe\n",
    "trainX = tf.transform(df.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1825)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if you are getting the correct output\n",
    "print(sum(trainX.toarray()[1]))\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the bag of words model, let's define the number of visible and hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define visible units\n",
    "visibleUnits = trainX.shape[1] # vocabulary size\n",
    "\n",
    "# assign number of units\n",
    "hiddenUnits = 5 # hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1825"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visibleUnits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility Functions\n",
    "\n",
    "# deine the sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1. / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleHiddenLayer(v_sample):\n",
    "    \n",
    "    # write the code for calculation of hPdf using Eq 4\n",
    "    hPdf = sigmoid(np.dot(v_sample, W)  + hiddenBias)\n",
    "    \n",
    "    # Here, np.random.binomial is used to create the hidden layer sample matrix\n",
    "    h_sample = np_rng.binomial(size=hPdf.shape, n=1, p=hPdf)\n",
    "    return [hPdf, h_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleVisibleLayer(h_sample, D):\n",
    "    \n",
    "    # complete the following function such that vPdf has the sum of entries equal to 1 for each of the datapoints in the batch\n",
    "    # you have to use axis = 1\n",
    "    numerator = np.exp(np.dot(h_sample, W.T) + visibleBias)\n",
    "    denominator = numerator.sum(axis=1).reshape((batchSize, 1))\n",
    "    vPdf = numerator/denominator\n",
    "    \n",
    "    # Here np.random.multinomial is used to sample as each document has different number of words \n",
    "    # and hence D is also a parameter in sampling    \n",
    "    v_sample = np.zeros((batchSize, vPdf.shape[1]))\n",
    "    for i in range(batchSize):\n",
    "        v_sample[i] = np_rng.multinomial(size=1, n=D[i], pvals=vPdf[i])\n",
    "    return [vPdf, v_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(h_sample, D):\n",
    "    \n",
    "    #use sampleVIsibleLayer and sampleHiddenLayer \n",
    "    vPdf, v_sample = sampleVisibleLayer(h_sample, D)\n",
    "    hPdf, h_sample = sampleHiddenLayer(v_sample)\n",
    "    return [vPdf, v_sample, hPdf, h_sample ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_k(data,k):\n",
    "    \n",
    "    D = data.sum(axis=1)\n",
    "    hiddenPDF_data, hiddenSample_data = sampleHiddenLayer(data)\n",
    "    chain_start = hiddenSample_data\n",
    "\n",
    "    for step in range(k):\n",
    "        if step == 0:\n",
    "            visiblePDF, visibleSamples, hiddenPDF, hiddenSamples  = gibbs(chain_start, D) \n",
    "        else:\n",
    "            visiblePDF, visibleSamples, hiddenPDF, hiddenSamples = gibbs(hiddenSamples, D)\n",
    "    return hiddenPDF_data, visibleSamples, hiddenPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "visibleUnits: no of words in your Bag of words Model\n",
    "hiddenUnits: no of topics\n",
    "batchSize: data slice to be selected \n",
    "epochs: no of iterations\n",
    "eta: learning rate\n",
    "mrate: momentum coefficient\n",
    "W : weights between the visible and hidden layer\n",
    "visibleBias, hiddenBias: biases for visible and hidden layer respectively\n",
    "\"\"\"\n",
    "\n",
    "# define batch size\n",
    "batchSize = 200\n",
    "\n",
    "epochs = 100\n",
    "eta = 0.05\n",
    "mrate = 0.5\n",
    "np_rng = np.random.RandomState(1234)\n",
    "\n",
    "# initialise weights\n",
    "weightinit=0.01\n",
    "W = weightinit * np_rng.randn(visibleUnits, hiddenUnits)\n",
    "visibleBias = weightinit * np_rng.randn(visibleUnits)\n",
    "hiddenBias = np.zeros((hiddenUnits))\n",
    "\n",
    "# interations of gibbs sampling\n",
    "k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataX,k):\n",
    "    # for momentum\n",
    "    mW = np.zeros((visibleUnits, hiddenUnits)) # initialise weights\n",
    "    mvisibleBias = np.zeros((visibleUnits)) # initialise visible biases with zeros\n",
    "    mhiddenBias = np.zeros((hiddenUnits)) # initialise hidden biases with zeros\n",
    "    global W,visibleBias,hiddenBias,mrate,batchSize,epochs\n",
    "    for epoch in range(epochs):\n",
    "        np_rng.shuffle(dataX) #shuffling the data\n",
    "        \n",
    "        for i in range(0, dataX.shape[0], batchSize):\n",
    "            mData = dataX[i:i + batchSize] #select a batch of datapoints\n",
    "            hiddenPDF_data, visibleSamples, hiddenPDF = cd_k(mData,k) #Contrastive Divergence on the batch for k iterations\n",
    "\n",
    "            mW = mW * mrate + (np.dot(mData.T, hiddenPDF_data) - np.dot(visibleSamples.T, hiddenPDF)) #calculate the update weight matrix\n",
    "            mvisibleBias = mvisibleBias * mrate + np.mean(mData - visibleSamples, axis=0) #calculate the update visible bias vector\n",
    "            mhiddenBias = mhiddenBias * mrate + np.mean(hiddenPDF_data - hiddenPDF, axis=0) #calculate the hidden bias vector\n",
    "\n",
    "            W += eta * mW #weight update equation\n",
    "            visibleBias += eta * mvisibleBias  #visible bias update equation\n",
    "            hiddenBias += eta * mhiddenBias  #hidden bias update equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(trainX.toarray(),k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worddist( topic, voc):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize every topic =1 once \n",
    "    \"\"\"\n",
    "    vecTopics = np.zeros((topic, topic))\n",
    "    for i in range(len(vecTopics)):\n",
    "        vecTopics[i][i] = 1\n",
    "    \n",
    "    \n",
    "    for i, vecTopic in enumerate(vecTopics):\n",
    "       \n",
    "        numerator = np.exp(np.dot(vecTopic, W.T) + visibleBias)\n",
    "        denominator = numerator.sum().reshape((1, 1))\n",
    "        word_distribution = (numerator/denominator).flatten()\n",
    "        \n",
    "        tmpDict = {}\n",
    "        for j in voc.keys():\n",
    "            tmpDict[j] = word_distribution[voc[j]]\n",
    "        print('topic', str(i), ':', vecTopic)\n",
    "        lm=0\n",
    "        for word, prob in sorted(tmpDict.items(), key=lambda x:x[1], reverse=True):\n",
    "            print ( word, str(prob))\n",
    "            lm+=1\n",
    "            if lm==15:\n",
    "                break\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : [1. 0. 0. 0. 0.]\n",
      "actually 0.02767914759045432\n",
      "pda 0.023380123859689444\n",
      "real 0.021818503727544132\n",
      "simple 0.015835207707529095\n",
      "drain 0.013295122822284653\n",
      "50 0.010538843387989572\n",
      "down 0.00891493652872779\n",
      "world 0.008891607408844055\n",
      "useful 0.008786481873627934\n",
      "neat 0.008339275610760949\n",
      "like 0.007613590028872028\n",
      "machine 0.007581361040531757\n",
      "stated 0.007124520268533332\n",
      "weak 0.006990490558752492\n",
      "usable 0.006977622291836275\n",
      "\n",
      "\n",
      "topic 1 : [0. 1. 0. 0. 0.]\n",
      "noise 0.03353141348056335\n",
      "piece 0.020420758749466503\n",
      "failed 0.014389161558974895\n",
      "hate 0.011527950767977302\n",
      "big 0.008602591898304191\n",
      "day 0.008295919124564023\n",
      "breaking 0.006703870631010143\n",
      "bose 0.0058457935946980245\n",
      "front 0.005601051426804026\n",
      "ear 0.005209908617927042\n",
      "most 0.005131890991124724\n",
      "jawbone 0.005086079477561938\n",
      "nyc 0.0049339380980908286\n",
      "size 0.004896378830246795\n",
      "protector 0.0048269046639245064\n",
      "\n",
      "\n",
      "topic 2 : [0. 0. 1. 0. 0.]\n",
      "had 0.04475881121860015\n",
      "headset 0.028756744944506925\n",
      "ve 0.018543849781484753\n",
      "so 0.01211628531025495\n",
      "use 0.011329708516872022\n",
      "but 0.011019145513814348\n",
      "few 0.009562213759473328\n",
      "best 0.009414197438483514\n",
      "be 0.007689064644501144\n",
      "from 0.00768468033502045\n",
      "out 0.007536599995521311\n",
      "bought 0.007337199594023797\n",
      "has 0.007278614287624086\n",
      "after 0.006654660559799732\n",
      "more 0.006446303443479837\n",
      "\n",
      "\n",
      "topic 3 : [0. 0. 0. 1. 0.]\n",
      "love 0.01828576244646722\n",
      "network 0.015065155695345177\n",
      "obviously 0.01026952941506495\n",
      "basic 0.010235754631951515\n",
      "wasted 0.00993019341824631\n",
      "samsung 0.009905169114875826\n",
      "excellent 0.00868478849074862\n",
      "sins 0.008584836780710103\n",
      "full 0.008351450516490016\n",
      "knows 0.008296907949243653\n",
      "service 0.008138551982738194\n",
      "study 0.007821998757344415\n",
      "terrible 0.007743026002728867\n",
      "development 0.006690767297541895\n",
      "its 0.006471003694272294\n",
      "\n",
      "\n",
      "topic 4 : [0. 0. 0. 0. 1.]\n",
      "work 0.037193102161799545\n",
      "anything 0.029117517982810592\n",
      "star 0.015044474501114612\n",
      "seller 0.012672702525601714\n",
      "nokia 0.012246104171815271\n",
      "cbr 0.010357961097195953\n",
      "mp3s 0.010239754965765345\n",
      "tech 0.010044378264171715\n",
      "preferably 0.009246645167757578\n",
      "home 0.008641218474376655\n",
      "windows 0.008517662060897374\n",
      "pocket 0.008400761908443848\n",
      "did 0.008381716053537788\n",
      "should 0.008195402740707295\n",
      "does 0.007669882801894136\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worddist( hiddenUnits, tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the probability assigned to each word for ever topic present."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
